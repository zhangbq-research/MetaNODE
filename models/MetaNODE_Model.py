import mathimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom utils import count_accuracyimport randomfrom torchdiffeq._impl.adjoint import odeint_adjointfrom torchdiffeq._impl.odeint import odeintdef one_hot(indices, depth):    encoded_indicies = torch.zeros(indices.size() + torch.Size([depth])).cuda()    index = indices.view(indices.size() + torch.Size([1]))    encoded_indicies = encoded_indicies.scatter_(1, index, 1)    return encoded_indiciesclass MultiHeadAttention(nn.Module):    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):        super().__init__()        self.n_head = n_head        self.d_k = d_k        self.d_v = d_v        self.w_qs = nn.Sequential(            nn.Linear(d_model, n_head * d_k, bias=True),            nn.ELU(inplace=True),        )        self.w_ks = nn.Sequential(            nn.Linear(d_model, n_head * d_k, bias=True),            nn.ELU(inplace=True),        )        self.w_vs = nn.Sequential(            nn.Linear(d_model, d_model, bias=True),            nn.ELU(inplace=True),            nn.BatchNorm1d(d_model, affine=True),            nn.Linear(d_model, n_head * d_v, bias=True),            nn.ELU(inplace=True),        )        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)        self.dropout = nn.Dropout(dropout)        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)    def forward(self, q, k, v, mask=None):        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)        residual = q        q = self.w_qs(q.reshape(-1, q.shape[-1])).view(sz_b, len_q, n_head, d_k)        k = self.w_ks(k.reshape(-1, k.shape[-1])).view(sz_b, len_k, n_head, d_k)        v = self.w_vs(v.reshape(-1, v.shape[-1])).view(sz_b, len_v, n_head, d_v)        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)        if mask is not None:            mask = mask.unsqueeze(1)        q, attn = self.attention(q, k, v, mask=mask)        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)        q = self.dropout(self.fc(q))        q += residual        q = self.layer_norm(q)        return q, attnclass ScaledDotProductAttention(nn.Module):    ''' Scaled Dot-Product Attention '''    def __init__(self, temperature, attn_dropout=0.1):        super().__init__()        self.temperature = temperature        self.dropout = nn.Dropout(attn_dropout)    def forward(self, q, k, v, mask=None):        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))        if mask is not None:            attn = attn.masked_fill(mask == 0, -1e9)        attn = self.dropout(F.softmax(attn, dim=-1))        output = torch.matmul(attn, v)        return output, attnclass DDirDiffOPTODEFunc(nn.Module):    def __init__(self, latent_dim=64, nhidden=1*64):        super(DDirDiffOPTODEFunc, self).__init__()        self.latent_dim = latent_dim        self.n_head = 4        self.n_time = 1        self.diff_embedding = []        for i in range(self.n_head):            diff_embedding_i = [nn.Sequential(            nn.Linear((nhidden+5) * 5, int(nhidden*(i+1)), bias=False),            nn.ELU(inplace=True),            nn.BatchNorm1d(int(nhidden*(i+1)), affine=True),            ) for _ in range(self.n_time)]            diff_embedding_ii = nn.ModuleList(diff_embedding_i)            self.diff_embedding.append(diff_embedding_ii)        self.diff_embedding = nn.ModuleList(self.diff_embedding)        self.diff_infer = []        for i_head in range(self.n_head):            diff_infer_i = [nn.Sequential(                nn.Linear(int(nhidden*(i_head+1)), nhidden, bias=False),            ) for _ in range(self.n_time)]            diff_infer_ii = nn.ModuleList(diff_infer_i)            self.diff_infer.append(diff_infer_ii)        self.diff_infer = nn.ModuleList(self.diff_infer)        self.inference_scale = nn.Sequential(                nn.Linear((nhidden) * 2, int(nhidden*1), bias=False),                nn.ELU(inplace=True),                nn.Linear(in_features=nhidden*1, out_features=1, bias=False),                nn.Sigmoid(),            )        self.atten = []        for i_head in range(self.n_head):            self.atten.append(MultiHeadAttention(n_head=8, d_model=int(nhidden*(i_head+1)), d_k=16, d_v=16, dropout=0.1))        self.atten = nn.ModuleList(self.atten)        self.nfe = 0        self.n_way = 5        self.prototype_labels = torch.LongTensor(list(range(self.n_way))).cuda()        self.prototype_labels = one_hot(self.prototype_labels, self.n_way)        self.diff_scale_init_1 = 0.1        self.scale_delay = 0.1        self.scale_step = 40        self.scale_factor = 10    def set_support(self, prototypes, support, support_labels, query, target_path):        tasks_per_batch = support.size(0)        n_support = support.size(1)        n_query = query.size(1)        self.support = support        self.support_labels = one_hot(support_labels.view(tasks_per_batch * n_support), self.n_way)        self.support_labels = self.support_labels.view(tasks_per_batch, n_support, -1)        self.query = query        self.query_labels = torch.ones(tasks_per_batch, n_query, self.n_way).type_as(self.support_labels)*1/self.n_way        self.prototypes_path = []        self.target_path = target_path    def forward(self, t, x):        self.nfe += 1        self.diff_scale = self.diff_scale_init_1 * torch.pow(self.scale_delay, (t / self.scale_step))        nb, nw = x.shape        prototypes = x.reshape(nb, self.n_way, -1)        prototype_labels = self.prototype_labels.unsqueeze(dim=0).expand(nb, -1, -1)        x = torch.cat([prototypes, prototype_labels], dim=-1)        support = torch.cat([self.support, self.support_labels], dim=-1)        query = torch.cat([self.query, self.query_labels], dim=-1)        for i in range(self.n_head):            left = self.support.unsqueeze(dim=1).expand(-1, self.n_way, -1, -1)            right = prototypes.unsqueeze(dim=2).expand(-1, -1, self.support.shape[1], -1)            data = torch.cat([left, right], dim=-1)            nb, nw, ns, nf = data.shape            data = data.reshape(-1, nf)            scale_support = 2 * self.inference_scale(data)            scale_support = scale_support.reshape(nb, nw, ns, -1)            diff_support = scale_support* self.support.unsqueeze(dim=1).expand(-1, self.n_way, -1, -1) - \                           prototypes.unsqueeze(dim=2).expand(-1, -1, self.support.shape[1], -1)            left = self.query.unsqueeze(dim=1).expand(-1, self.n_way, -1, -1)            right = prototypes.unsqueeze(dim=2).expand(-1, -1, self.query.shape[1], -1)            data = torch.cat([left, right], dim=-1)            nb, nw, ns, nf = data.shape            data = data.reshape(-1, nf)            scale_query = 2 * self.inference_scale(data)            scale_query = scale_query.reshape(nb, nw, ns, -1)            diff_query = scale_query*self.query.unsqueeze(dim=1).expand(-1, self.n_way, -1, -1) - \                           prototypes.unsqueeze(dim=2).expand(-1, -1, self.query.shape[1], -1)            diff_pos = torch.cat([diff_support, diff_query], dim=2)            all_x = torch.cat([support, query], dim=1)            # all_x = query            x_left = x.unsqueeze(dim=2).expand(-1, -1, all_x.shape[1], -1)            all_x_right = all_x.unsqueeze(dim=1).expand(-1, self.n_way, -1, -1)            new_x = torch.cat([x_left * all_x_right, x_left * x_left, all_x_right * all_x_right, x_left, all_x_right],                              dim=-1)            nb, nw, ns, nf = new_x.shape            new_x = new_x.reshape(-1, nf)            i_time = 0            diff_embedding = self.diff_embedding[i][i_time](new_x).squeeze(dim=-1)            diff_embedding = diff_embedding.reshape(nb, nw*ns, -1)            diff_embedding_js, _ = self.atten[i](diff_embedding, diff_embedding, diff_embedding)            diff_embedding_js = diff_embedding_js.reshape(-1, diff_embedding_js.shape[-1])            diff_weights = self.diff_infer[i][i_time](diff_embedding_js)            diff_weights = 1 * diff_weights.reshape(nb, nw, ns, -1)            diff_weights = F.softmax(diff_weights, dim=1)            topk, indices = torch.topk(diff_weights, 40, dim=2)            mask = torch.zeros_like(diff_weights)            mask = mask.scatter(2, indices, 1)            diff_weights = diff_weights * mask            diff_weights_pos = diff_weights / torch.sum(diff_weights, dim=2, keepdim=True)            diff_mean1 = torch.sum((diff_weights_pos * diff_pos), dim=2)            diff_diff = torch.pow(diff_pos - diff_mean1.unsqueeze(2).expand(-1, -1, diff_pos.shape[2], -1), 2)            diff_std1 = torch.sum((diff_weights_pos * diff_diff), dim=2)            if i == 0:                diff_mean = diff_mean1                diff_std = diff_std1            else:                diff_mean = (diff_std1 * diff_mean + diff_std * diff_mean1) / (diff_std1 + diff_std)                diff_std = (diff_std1 * diff_std) / (diff_std1 + diff_std)        diff = diff_mean * self.diff_scale        return diff.reshape(nb, -1)class DiffNODEOPTClassifier(nn.Module):    def __init__(self):        super().__init__()        self.func = DDirDiffOPTODEFunc(latent_dim=512, nhidden=512)        self.scale_factor = 10		self.update_step = 40        self.update_step_test = 40    def classify(self, data, paras):        logits = self.scale_factor * torch.nn.functional.cosine_similarity(            data.unsqueeze(2).expand(-1, -1, paras.shape[1], -1),            paras.unsqueeze(1).expand(-1, data.shape[1], -1, -1),            dim=-1)        return logits    def forward(self, query, support, support_labels, query_labels, n_way, n_shot, is_train=True, n_iter=1):        tasks_per_batch = query.size(0)        n_support = support.size(1)        assert (query.dim() == 3)        assert (support.dim() == 3)        assert (query.size(0) == support.size(0) and query.size(2) == support.size(2))        assert (n_support == n_way * n_shot)  # n_support must equal to n_way * n_shot        support_labels_one_hot = one_hot(support_labels.view(tasks_per_batch * n_support), n_way)        support_labels_one_hot = support_labels_one_hot.view(tasks_per_batch, n_support, n_way)        # ************************* Compute Prototypes **************************        labels_train_transposed = support_labels_one_hot.transpose(1, 2)        prototypes = torch.bmm(labels_train_transposed, support)        # Divide with the number of examples per novel category.        prototypes = prototypes.div(            labels_train_transposed.sum(dim=2, keepdim=True).expand_as(prototypes)        )        logits = []        logit = self.classify(query, prototypes)        logits.append(logit)        self.func.set_support(prototypes, support, support_labels, query, None)        batch_t = torch.tensor(list(range(0, self.update_step_test*1+1, 1))).float()        classifiers_1 = odeint_adjoint(self.func, prototypes.reshape(tasks_per_batch, -1), batch_t, method='rk4')        classifiers = classifiers_1[1:]        for i in range(1, self.update_step_test):            classifier = classifiers[i]            classifier = classifier.reshape(tasks_per_batch, n_way, -1)            logit = self.classify(query, classifier)            logits.append(logit)        return logits